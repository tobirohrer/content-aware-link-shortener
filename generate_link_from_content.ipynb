{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import html2text\n",
    "import requests\n",
    "import json\n",
    "import operator\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ELASTIC_URL = \"http://localhost:9200/\"\n",
    "json_header = {\"Content-Type\": \"application/json\"}\n",
    "MAX_URL_LENGTH = 10\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"https://towardsdatascience.com/data-augmentation-experimentation-3e274504f04b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "site_content = html2text.html2text(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(site_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "special_chars = {u'ä': 'ae', \n",
    "                 u'ü': 'ue', \n",
    "                 u'ß': 'ss', \n",
    "                 u'ö': 'oe'}\n",
    "\n",
    "for special_char in special_chars:\n",
    "    site_content = site_content.replace(unicode(special_char), special_chars[special_char])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(site_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk_tokens = word_tokenize(site_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleared_tokens = []\n",
    "\n",
    "custom_stopwords = set({'http', 'https', 'and','jpg', 'mehr', 'fuer', 'gif', 'der', 'die', 'das', 'wir', 'ihr', 'sie', 'es', 'und', 'in', 'den'})\n",
    "merged_stopwords = custom_stopwords.union(stopwords.words('german'))\n",
    "merged_stopwords = merged_stopwords.union(stopwords.words('english'))\n",
    "\n",
    "for nltk_token in nltk_tokens:\n",
    "    if nltk_token.isalnum() and nltk_token.lower() not in merged_stopwords and len(nltk_token) > 2:\n",
    "        cleared_tokens.append(nltk_token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cleared_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hits = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in cleared_tokens:\n",
    "    if token in hits:\n",
    "        hits[token] = hits[token] +1\n",
    "    else:\n",
    "        hits[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for hit in hits:\n",
    "    hits[hit] = hits[hit] / (len(hit)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_hits = sorted(hits.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_url = False\n",
    "i = 0\n",
    "j = 1\n",
    "repeat = 0\n",
    "while not found_url:\n",
    "    repeat = repeat +1\n",
    "    \n",
    "    if repeat > 7:\n",
    "        found_url = True\n",
    "        \n",
    "    url = sorted_hits[i][0] + \"-\" + sorted_hits[j][0]\n",
    "    url_reverse =  sorted_hits[j][0] + \"-\" + sorted_hits[i][0]\n",
    "    print(url)\n",
    "    print(url_reverse)\n",
    "    \n",
    "    if j-i > 3:\n",
    "        i = i + 1\n",
    "    else: \n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text , analyzer):\n",
    "    body = { \n",
    "        \"analyzer\": analyzer,\n",
    "        \"text\": text\n",
    "    }\n",
    "    response = requests.get(ELASTIC_URL + '_analyze', data=json.dumps(body),headers = json_header).json()\n",
    "    tokens = []\n",
    "    for token in response['tokens']:\n",
    "        tokens.append(token['token'])\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(site_content, \"german\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
